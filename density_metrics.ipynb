{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting to Augur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get('PG_USER'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psycopg2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msalc\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpsycopg2\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./config.json\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m config_file:\n\u001b[1;32m      7\u001b[0m     config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(config_file)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'psycopg2'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import sqlalchemy as salc\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "database_connection_string = 'postgresql+psycopg2://{}:{}@{}:{}/{}'.format(\n",
    "                              os.environ.get('PG_USER'),\n",
    "                              os.environ.get('PG_PASS'),\n",
    "                              os.environ.get('PG_HOST'),\n",
    "                              os.environ.get('PG_PORT'),\n",
    "                              os.environ.get('PG_DB')\n",
    "                            )\n",
    "\n",
    "dbschema='augur_data'\n",
    "engine = salc.create_engine(\n",
    "    database_connection_string,\n",
    "    connect_args={'options': '-csearch_path={}'.format(dbschema)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity Metrics\n",
    "- SQL query for github activity metrics\n",
    "- Focused on the increment in activity (star/fork/watch/committer/commit/issue) over time to prevent from old repo having lots of accumulated activity.\n",
    "    -  Assigned different weights for #increase_in_stars, #increase_in_forks, #increase_in_watch, #increase_in_committer, #increase_in_commit, #increase_in_issue, #increase_in_pr, #increase_in_pr_open, #increase_in_pr_close, #increase_in_pr_merge, then sum them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe = pd.DataFrame()\n",
    "\n",
    "repo_query = salc.sql.text(f\"\"\"\n",
    "SELECT x.repo_id,\n",
    "       x.rg_name,\n",
    "       x.repo_name,\n",
    "       last_updated,\n",
    "       DATE(last_updated),\n",
    "       to_char(last_updated, 'DAY'),\n",
    "       EXTRACT(year FROM last_updated) AS \"Year\",\n",
    "       EXTRACT(month FROM last_updated) AS \"month\",\n",
    "       extract(hour from last_updated) AS \"hour\",\n",
    "       x.increase_committer,\n",
    "       x.increase_pr_open,\n",
    "       x.increase_commit,\n",
    "       (x.increase_committer + x.increase_pr_open + x.increase_pr_close + x.increase_pr_merge + x.increase_issue + x.increase_pr + x.increase_star + x.increase_fork)*10 AS total\n",
    "            FROM(\n",
    "        SELECT \n",
    "            rg.repo_group_id,\n",
    "            rg.rg_name,\n",
    "            r.repo_id,\n",
    "            r.repo_name,\n",
    "            /*ri.license,*/\n",
    "            CASE\n",
    "                WHEN r.repo_id - lag(r.repo_id) over (order by rg.repo_group_id ASC, r.repo_id ASC, ri.last_updated) = 0 THEN \n",
    "                (ri.stars_count - lag(ri.stars_count) over (order by rg.repo_group_id ASC, r.repo_id ASC, ri.last_updated)) * 0.01\n",
    "            END\n",
    "                AS increase_star,\n",
    "            CASE\n",
    "                WHEN r.repo_id - lag(r.repo_id) over (order by rg.repo_group_id ASC, r.repo_id ASC, ri.last_updated) = 0 THEN \n",
    "                (ri.fork_count - lag(ri.fork_count) over (order by rg.repo_group_id ASC, r.repo_id ASC, ri.last_updated)) * 0.2\n",
    "            END\n",
    "                AS increase_fork,\n",
    "            CASE\n",
    "                WHEN r.repo_id - lag(r.repo_id) over (order by rg.repo_group_id ASC, r.repo_id ASC, ri.last_updated) = 0 THEN \n",
    "                (ri.watchers_count - lag(ri.watchers_count) over (order by rg.repo_group_id ASC, r.repo_id ASC, ri.last_updated)) * 0.1\n",
    "            END\n",
    "                AS increase_watch,\n",
    "            CASE\n",
    "                WHEN r.repo_id - lag(r.repo_id) over (order by rg.repo_group_id ASC, r.repo_id ASC, ri.last_updated) = 0 THEN \n",
    "                (ri.committers_count - lag(ri.committers_count) over (order by rg.repo_group_id ASC, r.repo_id ASC, ri.last_updated)) *1.6\n",
    "            END\n",
    "                AS increase_committer,\n",
    "            CASE\n",
    "                WHEN r.repo_id - lag(r.repo_id) over (order by rg.repo_group_id ASC, r.repo_id ASC, ri.last_updated) = 0 THEN \n",
    "                (ri.commit_count - lag(ri.commit_count) over (order by rg.repo_group_id ASC, r.repo_id ASC, ri.last_updated)) * 1.3\n",
    "            END\n",
    "                AS increase_commit,\n",
    "            CASE\n",
    "                WHEN r.repo_id - lag(r.repo_id) over (order by rg.repo_group_id ASC, r.repo_id ASC, ri.last_updated) = 0 THEN \n",
    "                (ri.issues_count - lag(ri.issues_count) over (order by rg.repo_group_id ASC, r.repo_id ASC, ri.last_updated)) * 0.5\n",
    "            END\n",
    "                AS increase_issue,\n",
    "            CASE\n",
    "                WHEN r.repo_id - lag(r.repo_id) over (order by rg.repo_group_id ASC, r.repo_id ASC, ri.last_updated) = 0 THEN \n",
    "                (ri.pull_request_count - lag(ri.pull_request_count) over (order by rg.repo_group_id ASC, r.repo_id ASC, ri.last_updated)) * 1\n",
    "            END\n",
    "                AS increase_pr,\n",
    "            CASE\n",
    "                WHEN r.repo_id - lag(r.repo_id) over (order by rg.repo_group_id ASC, r.repo_id ASC, ri.last_updated) = 0 THEN \n",
    "                (ri.pull_requests_open - lag(ri.pull_requests_open) over (order by rg.repo_group_id ASC, r.repo_id ASC, ri.last_updated)) * 1.2\n",
    "            END\n",
    "                AS increase_pr_open,\n",
    "            CASE\n",
    "                WHEN r.repo_id - lag(r.repo_id) over (order by rg.repo_group_id ASC, r.repo_id ASC, ri.last_updated) = 0 THEN \n",
    "                (ri.pull_requests_closed - lag(ri.pull_requests_closed) over (order by rg.repo_group_id ASC, r.repo_id ASC, ri.last_updated)) * 1.5\n",
    "            END\n",
    "                AS increase_pr_close,\n",
    "            CASE\n",
    "                WHEN r.repo_id - lag(r.repo_id) over (order by rg.repo_group_id ASC, r.repo_id ASC, ri.last_updated) = 0 THEN \n",
    "                (ri.pull_requests_merged - lag(ri.pull_requests_merged) over (order by rg.repo_group_id ASC, r.repo_id ASC, ri.last_updated)) * 1.8    \n",
    "            END\n",
    "                AS increase_pr_merge,\n",
    "            ri.last_updated,\n",
    "            CASE\n",
    "                WHEN EXTRACT(YEAR FROM ri.last_updated) < 2022 THEN 'far away'\n",
    "                WHEN EXTRACT(YEAR FROM ri.last_updated) >= 2022 THEN 'recent'\n",
    "            END\n",
    "                AS segment,\n",
    "            EXTRACT(year FROM last_updated) AS \"Year\",\n",
    "            EXTRACT(month FROM last_updated) AS \"month\" \n",
    "        FROM REPO r\n",
    "            LEFT JOIN repo_groups rg\n",
    "            ON rg.repo_group_id = r.repo_group_id\n",
    "            LEFT join repo_info ri \n",
    "            on r.repo_id = ri.repo_id \n",
    "        /*where rg.rg_name = 'agroal'*/\n",
    "        order by rg.repo_group_id ASC, r.repo_id ASC, ri.last_updated) AS x\n",
    "\"\"\")\n",
    "\n",
    "dframe = pd.read_sql(repo_query, con=engine)\n",
    "dframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill all NA value into zero\n",
    "dframe = dframe.fillna(0)\n",
    "# calculating activeness percentage based on org and repo_name\n",
    "df2 = dframe.groupby(['rg_name', 'repo_name']).agg({'total': 'sum'})\n",
    "df3 = df2.groupby(level=0).apply(lambda x:100 * x / float(x.sum()))\n",
    "df4 = df3['total'].to_frame().reset_index()\n",
    "df4 = df4[df4['total'] != 0.0]\n",
    "df4.head()\n",
    "\n",
    "# extract column 'month' and 'percentage', copy it as another dataframe\n",
    "# dft = dtest[['month', 'percentage']].copy()\n",
    "# dft = dframe['percentage'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df4[df4['rg_name'] == 'openshift']\n",
    "t[t['repo_name'] == 'docker-distribution']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(df4, x=\"rg_name\", y=\"total\", color=\"repo_name\", text=\"repo_name\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pie chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drepo = df4[df4['rg_name'] == 'kubernetes']\n",
    "\n",
    "import plotly.express as px\n",
    "fig_pie = px.pie(data_frame=drepo, names='repo_name', values='total')\n",
    "fig_pie.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot for org Density by Activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe = dframe.fillna(0)\n",
    "# calculating activeness percentage based on org and repo_name\n",
    "df2 = dframe.groupby(['rg_name', 'repo_name']).agg({'total': 'sum'})\n",
    "ho = df2['total'].to_frame().reset_index()\n",
    "\n",
    "\n",
    "hoo = ho.groupby(['rg_name']).agg({'total': 'sum'})\n",
    "drank = hoo.reset_index()\n",
    "drank.sort_values(by = 'total', ascending=False).reset_index()\n",
    "drank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(drank, x=\"total\", y=\"rg_name\", orientation='h')\n",
    "fig.update_layout(yaxis={'categoryorder':'total ascending'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drank[drank['rg_name'] == 'GNOME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity\n",
    "dframe_msg = pd.DataFrame()\n",
    "\n",
    "message_query = salc.sql.text(f\"\"\"\n",
    "    SELECT x.repo_id,\n",
    "            x.repo_name,\n",
    "            x.timeyear,\n",
    "            x.timemonth,\n",
    "            x.timedate,\n",
    "            COUNT(*) AS num_of_comment\n",
    "    FROM(\n",
    "        SELECT m.repo_id, \n",
    "                r.repo_name,\n",
    "                EXTRACT (year FROM msg_timestamp) AS timeyear,\n",
    "                EXTRACT (month FROM msg_timestamp) AS timemonth,\n",
    "                EXTRACT (day FROM msg_timestamp) AS timedate\n",
    "                FROM message m \n",
    "            left join repo r \n",
    "            on m.repo_id = r.repo_id \n",
    "            where m.repo_id is not null\n",
    "            limit 1000\n",
    "        ) AS x\n",
    "    GROUP BY x.repo_id, x.repo_name, x.timeyear, x.timemonth, x.timedate\n",
    "    ORDER BY x.repo_id, x.timeyear, x.timemonth, x.timedate\n",
    "\"\"\")\n",
    "\n",
    "dframe_msg = pd.read_sql(message_query, con=engine)\n",
    "dframe_msg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe_msg = dframe_msg.fillna(0)\n",
    "print(len(dframe_msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(dframe_msg, x = 'timeyear', y = 'num_of_comment', color='repo_name',\n",
    "                labels={'num_of_comment': 'Number Of Comments'},\n",
    "                height = 400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate number of comments for a repo by identifying unique contributor and within what timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe_unq_msg_cntrb = pd.DataFrame()\n",
    "\n",
    "unq_msg_cntrb_query = salc.sql.text(f\"\"\"\n",
    "\tSELECT x.repo_id,\n",
    "\t\t\tx.repo_name,\n",
    "\t\t\tx.datetime,\n",
    "\t\t\tCOUNT(x.msg_text) AS num_of_comments,\n",
    "\t\t\tCOUNT(distinct x.cntrb_id) AS num_of_unique_msg_cntrb\n",
    "\t\tFROM(\n",
    "\t\t\tSELECT m.repo_id,\n",
    "\t\t\t\t\tr.repo_name,\n",
    "\t\t\t\t\tm.cntrb_id,\n",
    "\t\t\t\t\tm.msg_text,\n",
    "\t\t\t\t\tTO_CHAR(msg_timestamp  :: DATE, 'yyyy-mm-dd') AS datetime\n",
    "\t\t\tFROM message m\n",
    "\t\t\tLEFT JOIN repo r\n",
    "\t\t\t\tON m.repo_id = r.repo_id\n",
    "\t\t\tWHERE m.repo_id is not null\n",
    "\t\t\tORDER BY m.repo_id, datetime\n",
    "\t\t\t) AS x\n",
    "\t\tGROUP BY x.datetime, x.repo_id, x.repo_name\n",
    "\tORDER BY x.repo_id, x.datetime\n",
    "\"\"\")\n",
    "\n",
    "dframe_unq_msg_cntrb = pd.read_sql(unq_msg_cntrb_query, con=engine)\n",
    "dframe_unq_msg_cntrb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = dframe_unq_msg_cntrb[dframe_unq_msg_cntrb['repo_id'] == 24441]\n",
    "dtest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Comments V.S. Number of Unique Comment Contributors by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "benchmark_value = dtest['num_of_comments'].mean()\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    x=dtest['datetime'],\n",
    "    y=dtest['num_of_comments'],\n",
    "    name='Number Of Comments',\n",
    "    marker_color='indianred'\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    x=dtest['datetime'],\n",
    "    y=dtest['num_of_unique_msg_cntrb'],\n",
    "    name='Number Of Unique Contributors',\n",
    "    marker_color='lightsalmon'\n",
    "))\n",
    "\n",
    "# Here we modify the tickangle of the xaxis, resulting in rotated labels.\n",
    "fig.add_hline(y=benchmark_value, annotation_text=f\"{round(benchmark_value, 2)}: Avg for # of comments\", line_dash='dot', annotation_font_size=20)\n",
    "fig.add_hline(y=dtest['num_of_unique_msg_cntrb'].mean(), annotation_text=f\"{round(dtest['num_of_unique_msg_cntrb'].mean(),2)}: Avg for # of unique comment contributors\")\n",
    "fig.update_layout(barmode='group', xaxis_tickangle=-45, title='Number of Comments V.S. Number of Unique Comment Contributors by day - operate-first-twitter repo')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe_unq_msg_cntrb = pd.DataFrame()\n",
    "\n",
    "unq_msg_cntrb_query = salc.sql.text(f\"\"\"\n",
    "\tSELECT x.repo_id,\n",
    "\t\t\tx.repo_name,\n",
    "\t\t\tx.yearmonth,\n",
    "\t\t\tCOUNT(x.msg_text) AS num_of_comments,\n",
    "\t\t\tCOUNT(distinct x.cntrb_id) AS num_of_unique_msg_cntrb\n",
    "\t\tFROM(\n",
    "\t\t\tSELECT m.repo_id,\n",
    "\t\t\t\t\tr.repo_name,\n",
    "\t\t\t\t\tm.cntrb_id,\n",
    "\t\t\t\t\tm.msg_text,\n",
    "\t\t\t\t\tCAST(EXTRACT(YEAR FROM msg_timestamp) AS text) || '-' || CAST(EXTRACT(MONTH FROM msg_timestamp) AS text) AS yearmonth\n",
    "\t\t\tFROM message m\n",
    "\t\t\tLEFT JOIN repo r\n",
    "\t\t\t\tON m.repo_id = r.repo_id\n",
    "\t\t\tWHERE m.repo_id is not null\n",
    "\t\t\tORDER BY m.repo_id, yearmonth\n",
    "\t\t\t) AS x\n",
    "\t\tGROUP BY x.repo_id, x.repo_name, x.yearmonth\n",
    "\tORDER BY x.repo_id, x.yearmonth\n",
    "\"\"\")\n",
    "\n",
    "dframe_unq_msg_cntrb = pd.read_sql(unq_msg_cntrb_query, con=engine)\n",
    "dframe_unq_msg_cntrb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhey = dframe_unq_msg_cntrb[dframe_unq_msg_cntrb['repo_name'] == 'kubernetes']\n",
    "dhey.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Comments V.S. Number of Unique Comment Contributors by month -- Kubernetes repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_value = dhey['num_of_comments'].mean()\n",
    "dhey['benchmark_score'] = dhey['num_of_comments'] - benchmark_value\n",
    "benchmark_value\n",
    "dhey.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "benchmark_value = dhey['num_of_comments'].mean()\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    x=dhey['yearmonth'],\n",
    "    y=dhey['num_of_comments'],\n",
    "    name='Number Of Comments',\n",
    "    marker_color='indianred'\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    x=dhey['yearmonth'],\n",
    "    y=dhey['num_of_unique_msg_cntrb'],\n",
    "    name='Number Of Unique Contributors',\n",
    "    marker_color='lightsalmon'\n",
    "))\n",
    "\n",
    "# Here we modify the tickangle of the xaxis, resulting in rotated labels.\n",
    "fig.add_hline(y=benchmark_value, annotation_text=f\" {round(benchmark_value,2)}: Avg for # of comments\", line_dash='dot', annotation_font_size=20)\n",
    "fig.add_hline(y=dhey['num_of_unique_msg_cntrb'].mean(), annotation_text= f\" {round(dhey['num_of_unique_msg_cntrb'].mean(),2)} : Avg for # of unique comment contributors\")\n",
    "fig.update_layout(barmode='group', xaxis_tickangle=-45, title='Number of Comments V.S. Number of Unique Comment Contributors by month - Kubernetes repo')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community\n",
    "### Contributors\n",
    "Defined as committers and authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pr_committers = pd.DataFrame()\n",
    "df_pr_authors = pd.DataFrame()\n",
    "\n",
    "committer_query = salc.sql.text(f\"\"\"\n",
    "    SELECT x.rg_name,\n",
    "            x.repo_id,\n",
    "            x.repo_name,\n",
    "            x.cmt_committer_date,\n",
    "            COUNT(x.cmt_id) AS num_of_commit,\n",
    "            COUNT(DISTINCT x.cmt_committer_raw_email) AS num_of_unique_commit\n",
    "        FROM(\n",
    "            SELECT rg.rg_name,\n",
    "                    c.repo_id,\n",
    "                    r.repo_name,\n",
    "                    c.cmt_id,\n",
    "                    c.cmt_committer_raw_email,\n",
    "                    c.cmt_committer_date \n",
    "            FROM commits c \n",
    "            LEFT JOIN repo r\n",
    "                ON c.repo_id = r.repo_id\n",
    "            left join repo_groups rg\n",
    "                on r.repo_group_id = rg.repo_group_id\n",
    "            WHERE c.repo_id is not null\n",
    "                AND c.cmt_committer_date >= '2020-01-01'\n",
    "            ORDER BY r.repo_group_id, c.repo_id, c.cmt_committer_date\n",
    "            ) as x\n",
    "        GROUP BY x.cmt_committer_date, x.rg_name, x.repo_id, x.repo_name\n",
    "    ORDER BY x.rg_name, x.repo_id, x.cmt_committer_date\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "author_query = salc.sql.text(f\"\"\"\n",
    "    SELECT x.rg_name,\n",
    "            x.repo_id,\n",
    "            x.repo_name,\n",
    "            x.cmt_author_date,\n",
    "            COUNT(x.cmt_id) AS num_of_author,\n",
    "            COUNT(DISTINCT x.cmt_author_raw_email) AS num_of_unique_author\n",
    "        FROM(\n",
    "            SELECT rg.rg_name,\n",
    "                    c.repo_id,\n",
    "                    r.repo_name,\n",
    "                    c.cmt_id,\n",
    "                    c.cmt_author_raw_email,\n",
    "                    c.cmt_author_date \n",
    "            FROM commits c \n",
    "            LEFT JOIN repo r\n",
    "                ON c.repo_id = r.repo_id\n",
    "            left join repo_groups rg\n",
    "                on r.repo_group_id = rg.repo_group_id\n",
    "            WHERE c.repo_id is not null\n",
    "                AND c.cmt_committer_date >= '2020-01-01'\n",
    "            ORDER BY r.repo_group_id, c.repo_id, c.cmt_author_date\n",
    "            ) as x\n",
    "        GROUP BY x.cmt_author_date, x.rg_name, x.repo_id, x.repo_name\n",
    "    ORDER BY x.rg_name, x.repo_id, x.cmt_author_date\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pr_committers = pd.read_sql(committer_query, con=engine)\n",
    "df_pr_committers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pr_authors = pd.read_sql(author_query, con=engine)\n",
    "df_pr_authors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of committers V.S. Number of unique committers over time -- Operate First Twitter repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcommit = df_pr_committers[df_pr_committers['rg_name'] == 24441]\n",
    "dcommit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "benchmark_value = df_pr_committers['num_of_commit'].mean()\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df_pr_committers['cmt_committer_date'],\n",
    "    y=df_pr_committers['num_of_commit'],\n",
    "    name='Number Of Committers',\n",
    "    base='repo_name',\n",
    "    marker_color='indianred'\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df_pr_committers['cmt_committer_date'],\n",
    "    y=df_pr_committers['num_of_unique_commit'],\n",
    "    name='Number Of Unique Committers',\n",
    "    base='repo_name',\n",
    "    marker_color='lightsalmon'\n",
    "))\n",
    "\n",
    "# Here we modify the tickangle of the xaxis, resulting in rotated labels.\n",
    "fig.add_hline(y=benchmark_value, annotation_text=f\"{round(benchmark_value, 2)}: Avg for # of Committers\", line_dash='dot', annotation_font_size=20)\n",
    "fig.add_hline(y=df_pr_committers['num_of_unique_commit'].mean(), annotation_text=f\"{round(df_pr_committers['num_of_unique_commit'].mean(),2)}: Avg for # of unique Committers\")\n",
    "fig.update_layout(barmode='group', xaxis_tickangle=-45, title='Number of Committers V.S. Number of Unique Committers Contributors by day - operate-first-twitter repo')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Authors V.S. Number of unique Authors over time -- Operate First Twitter repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dauthor = df_pr_authors[df_pr_authors['repo_id'] == 24441]\n",
    "dauthor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "benchmark_value = dauthor['num_of_author'].mean()\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    x=dauthor['cmt_author_date'],\n",
    "    y=dauthor['num_of_author'],\n",
    "    name='Number Of Authors',\n",
    "    marker_color='indianred'\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    x=dauthor['cmt_author_date'],\n",
    "    y=dauthor['num_of_unique_author'],\n",
    "    name='Number Of Unique Authors',\n",
    "    marker_color='lightsalmon'\n",
    "))\n",
    "\n",
    "# Here we modify the tickangle of the xaxis, resulting in rotated labels.\n",
    "fig.add_hline(y=benchmark_value, annotation_text=f\"{round(benchmark_value, 2)}: Avg for # of Authors\", line_dash='dot', annotation_font_size=20)\n",
    "fig.add_hline(y=dauthor['num_of_unique_author'].mean(), annotation_text=f\"{round(dauthor['num_of_unique_author'].mean(),2)}: Avg for # of unique Authors\")\n",
    "fig.update_layout(barmode='group', xaxis_tickangle=-45, title='Number of Authors V.S. Number of Unique Authors Contributors by day - operate-first-twitter repo')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance\n",
    "## PR\n",
    "SQL query for github performance metrics\n",
    "- whether the PR is closed or open -> status\n",
    "- time required to close an PR -> duration\n",
    "- how many days has passed since the ticket is closed -> exp decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe_pr = pd.DataFrame()\n",
    "\n",
    "pr_query = salc.sql.text(f\"\"\"\n",
    "/*\n",
    "1. whether the PR is closed or open -> status\n",
    "2. time required to close an PR -> duration\n",
    "3. Only the data from 2021 (?)\n",
    "4. how many days has passed since the ticket is closed -> exp decay\n",
    "*/\n",
    "SELECT x.repo_group_id,\n",
    "\t\trg.rg_name,\n",
    "        x.repo_id,\n",
    "        x.repo_name,\n",
    "\t\tx.close_duration,\n",
    "\t\tx.exp_decay,\n",
    "\t\tcount(pull_request_id) AS num,\n",
    "\t\tclose_duration*count(pull_request_id) + exp_decay*count(pull_request_id) as total\n",
    "\t\tFROM(\n",
    "\t\t\tSELECT pull_request_id,\n",
    "                    pull_requests.repo_id,\n",
    "                    r.repo_name,\n",
    "                    r.repo_group_id,\n",
    "\t\t\t\t    pr_src_state,\n",
    "\t\t\t\t    (pr_closed_at - pr_created_at) AS pull_request_duration,\n",
    "\t\t\t\t   CASE \n",
    "\t\t\t\t   \tWHEN pull_requests.pr_closed_at - pull_requests.pr_created_at <= INTERVAL '15 days' THEN 1\n",
    "\t\t\t\t   \tWHEN pull_requests.pr_closed_at - pull_requests.pr_created_at <= INTERVAL '30 days' THEN 0.66\n",
    "\t\t\t\t   \tWHEN pull_requests.pr_closed_at - pull_requests.pr_created_at <= INTERVAL '60 days' THEN 0.33\n",
    "\t\t\t\t   \tWHEN pull_requests.pr_closed_at - pull_requests.pr_created_at > INTERVAL '90 days' THEN 0.1\n",
    "\t\t\t\t   \tWHEN pull_requests.pr_closed_at IS NULL AND NOW() - pull_requests.pr_created_at < INTERVAL '45 days' THEN 0.5\n",
    "\t\t\t\t   \tELSE 0\n",
    "\t\t\t\t   END\n",
    "\t\t\t\t   AS close_duration,\n",
    "\t\t\t\t   NOW() - pull_requests.pr_closed_at AS \"time_passed\",\n",
    "\t\t\t\t   CASE \n",
    "\t\t\t\t   \tWHEN NOW() - pull_requests.pr_closed_at < INTERVAL '30 days' THEN 1\n",
    "\t\t\t\t   \tWHEN NOW() - pull_requests.pr_closed_at < INTERVAL '60 days' THEN 0.9\n",
    "\t\t\t\t   \tWHEN NOW() - pull_requests.pr_closed_at < INTERVAL '90 days' THEN 0.9*0.9\n",
    "\t\t\t\t   \tWHEN NOW() - pull_requests.pr_closed_at < INTERVAL '120 days' THEN 0.9*0.9*0.9\n",
    "\t\t\t\t   \tWHEN NOW() - pull_requests.pr_closed_at < INTERVAL '150 days' THEN 0.9*0.9*0.9*0.9\n",
    "\t\t\t\t   \tWHEN NOW() - pull_requests.pr_closed_at < INTERVAL '180 days' THEN 0.9*0.9*0.9*0.9*0.9\n",
    "\t\t\t\t   \tWHEN NOW() - pull_requests.pr_closed_at < INTERVAL '210 days' THEN 0.9*0.9*0.9*0.9*0.9*0.9\n",
    "\t\t\t\t   \tWHEN NOW() - pull_requests.pr_closed_at < INTERVAL '240 days' THEN 0.9*0.9*0.9*0.9*0.9*0.9*0.9\n",
    "\t\t\t\t   \tWHEN NOW() - pull_requests.pr_closed_at < INTERVAL '270 days' THEN 0.9*0.9*0.9*0.9*0.9*0.9*0.9*0.9\n",
    "\t\t\t\t   \tWHEN NOW() - pull_requests.pr_closed_at < INTERVAL '300 days' THEN 0.9*0.9*0.9*0.9*0.9*0.9*0.9*0.9*0.9\n",
    "\t\t\t\t   \tWHEN NOW() - pull_requests.pr_closed_at < INTERVAL '330 days' THEN 0.9*0.9*0.9*0.9*0.9*0.9*0.9*0.9*0.9*0.9\n",
    "\t\t\t\t   \tWHEN NOW() - pull_requests.pr_closed_at < INTERVAL '360 days' THEN 0.9*0.9*0.9*0.9*0.9*0.9*0.9*0.9*0.9*0.9*0.9\n",
    "\t\t\t\t   \tELSE 0\n",
    "\t\t\t\t   END\n",
    "\t\t\t\t   AS exp_decay,\n",
    "\t\t\t\t   pull_requests.pr_closed_at\n",
    "\t\t\tFROM pull_requests\n",
    "\t\t\t/*WHERE EXTRACT(YEAR FROM pull_requests.pr_closed_at) >= 2022*/\n",
    "            LEFT JOIN repo r\n",
    "                ON r.repo_id = pull_requests.repo_id\n",
    "\t\t\tORDER BY repo_id \n",
    "\t\t) AS x\n",
    "\t\tleft join repo_groups rg \n",
    "\t\t\ton rg.repo_group_id = x.repo_group_id\n",
    "\tGROUP BY x.repo_id, x.repo_name, x.repo_group_id, rg.rg_name, close_duration, exp_decay\n",
    "\torder by x.repo_id\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "dframe_pr = pd.read_sql(pr_query, con=engine)\n",
    "\n",
    "dframe_pr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe_pr_g = dframe_pr.groupby('repo_id')['total'].sum()\n",
    "dframe_pr_g.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issue\n",
    "- SQL query for github performance metrics\n",
    "- time required to close an issue -> duration\n",
    "- how many days has passed since the ticket is closed -> exp decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_query = salc.sql.text(f\"\"\"\n",
    "SELECT repo_id,\n",
    "\t\tclose_duration,\n",
    "\t\texp_decay,\n",
    "\t\tcount(issue_id) AS num,\n",
    "\t\tclose_duration*exp_decay*count(issue_id) AS total\n",
    "\t\tFROM(\n",
    "\t\t\tSELECT repo_id,\n",
    "\t\t\t\t   issue_id,\n",
    "\t\t\t\t   issue_state,\n",
    "\t\t\t\t   (closed_at - created_at) AS issue_close_duration,\n",
    "\t\t\t\t   /*(updated_at - created_at) as issue_update_duration,*/\n",
    "\t\t\t\t   CASE \n",
    "\t\t\t\t\t   \tWHEN i.closed_at - i.created_at <= interval '30 days' THEN 1\n",
    "\t\t\t\t\t   \tWHEN i.closed_at - i.created_at <= interval '60 days' THEN 0.66\n",
    "\t\t\t\t\t   \tWHEN i.closed_at - i.created_at <= interval '90 days' THEN 0.33\n",
    "\t\t\t\t\t   \tWHEN i.closed_at - i.created_at > interval '90 days' THEN 0.1\n",
    "\t\t\t\t\t\t/* the issue that has recently been opened*/\n",
    "\t\t\t\t\t   \twhen i.closed_at IS NULL AND NOW() - i.created_at < interval '45 days' THEN 0.5\n",
    "\t\t\t\t\t   \tELSE 0\n",
    "\t\t\t\t   END\n",
    "\t\t\t\t   AS close_duration,\n",
    "\t\t\t\t   i.created_at,\n",
    "\t\t\t\t   i.closed_at,\n",
    "\t\t\t\t   NOW() - i.closed_at AS \"time_passed_after_closing\",\n",
    "\t\t\t\t   CASE \n",
    "\t\t\t\t   \tWHEN NOW() - i.closed_at < interval '30 days' THEN 1\n",
    "\t\t\t\t   \tWHEN NOW() - i.closed_at < interval '60 days' THEN 0.9\n",
    "\t\t\t\t   \tWHEN NOW() - i.closed_at < interval '90 days' THEN 0.9*0.9\n",
    "\t\t\t\t   \tWHEN NOW() - i.closed_at < interval '120 days' THEN 0.9*0.9*0.9\n",
    "\t\t\t\t   \tWHEN NOW() - i.closed_at < interval '150 days' THEN 0.9*0.9*0.9*0.9\n",
    "\t\t\t\t   \tWHEN NOW() - i.closed_at < interval '180 days' THEN 0.9*0.9*0.9*0.9*0.9\n",
    "\t\t\t\t   \tWHEN NOW() - i.closed_at < interval '210 days' THEN 0.9*0.9*0.9*0.9*0.9*0.9\n",
    "\t\t\t\t   \tWHEN NOW() - i.closed_at < interval '240 days' THEN 0.9*0.9*0.9*0.9*0.9*0.9*0.9\n",
    "\t\t\t\t   \tWHEN NOW() - i.closed_at < interval '270 days' THEN 0.9*0.9*0.9*0.9*0.9*0.9*0.9*0.9\n",
    "\t\t\t\t   \tWHEN NOW() - i.closed_at < interval '300 days' THEN 0.9*0.9*0.9*0.9*0.9*0.9*0.9*0.9*0.9\n",
    "\t\t\t\t   \tWHEN NOW() - i.closed_at < interval '330 days' THEN 0.9*0.9*0.9*0.9*0.9*0.9*0.9*0.9*0.9*0.9\n",
    "\t\t\t\t   \tWHEN NOW() - i.closed_at < interval '360 days' THEN 0.9*0.9*0.9*0.9*0.9*0.9*0.9*0.9*0.9*0.9*0.9\n",
    "\t\t\t\t   \tELSE 0\n",
    "\t\t\t\t   END\n",
    "\t\t\t\t   AS exp_decay\n",
    "\t\t\tFROM issues i\n",
    "\t\t\tWHERE extract(year from i.created_at) >= 2022\n",
    "\t\t\tORDER BY repo_id\n",
    "\t\t\t) AS x\n",
    "\tGROUP BY repo_id, close_duration, exp_decay\n",
    "\torder by repo_id\n",
    "\"\"\")\n",
    "\n",
    "dframe_issue = pd.read_sql(issue_query, con=engine)\n",
    "\n",
    "dframe_issue.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe_issue_g = dframe_issue.groupby('repo_id')['total'].sum()\n",
    "dframe_issue_g.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "- Evaluating the duration from issue open to close as issue_close_duration column\n",
    "- Issue_close_duration_day column: extract day from issue_close_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue2_query = salc.sql.text(f\"\"\"\t\n",
    "\t\t\tSELECT repo_id,\n",
    "\t\t\t\t   issue_state,\n",
    "\t\t\t\t   (closed_at - created_at) AS issue_close_duration,\n",
    "\t\t\t\t   extract(day from closed_at - created_at) AS issue_close_duration_day,\n",
    "\t\t\t\t   comment_count\n",
    "\t\t\tFROM issues i\n",
    "\t\t\tWHERE extract(year from i.created_at) >= 2022\n",
    "\t\t\tORDER BY repo_id\n",
    "\"\"\")\n",
    "\n",
    "dtest = pd.read_sql(issue2_query, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract only issue_close_duration_day and comment_count column from the original dataframe and make a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = dtest[['issue_close_duration_day', 'comment_count']].copy()\n",
    "frame = frame.fillna(0)\n",
    "frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow method...\n",
    "Apply KMenas clustering on the duration_day and comment_count column for segmentation on issue being closed within how many days should be segmented into how many differenct groups, what are each decay rate...\n",
    "- Number of clusters (n = 4): number of groups for different performance (active performance/ mild performance/ poor performance/ low performance)\n",
    "- Cluster size (179995, 13545, 3011, 103): The threshold for the four groups?\n",
    "- Decay rate (slope in the curve?): how much weight should be given to active performance? Mild performance? Poor performance? Low performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(1,12)).fit(frame)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4, init='k-means++', random_state=0).fit(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.labels_)\n",
    "print(kmeans.inertia_)\n",
    "print(kmeans.n_iter_)\n",
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-means clustering has four clusters of size 179995, 13545, 3011, and 103, which in our case menas that there's 179995 active performance, 13545 mild performace, and 3011 poor performance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.scatterplot(data=frame, x=\"issue_close_duration_day\", y=\"comment_count\", hue=kmeans.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: I used commnet count as y axis, but not sure if that make sense at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity plot (WIP)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dft = dframe[['rg_name', 'last_updated', 'percentage']].copy()\n",
    "dft.head()\n",
    "dft['perc_'] = dft.apply(lambda row : row[2]*100, axis=1)\n",
    "dft"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Plot...\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import july\n",
    "from july.utils import date_range\n",
    "\n",
    "dates = date_range('2021-10-12', '2022-07-01')\n",
    "data = dft['perc_'].to_numpy()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# GitHub Activity like plot (for someone with consistently random work patterns).\n",
    "july.heatmap(dates, data, title='Github Activity', cmap=\"github\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
